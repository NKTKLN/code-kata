{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d34ba7a",
   "metadata": {},
   "source": [
    "# 🧪 Custom Bayes Search\n",
    "\n",
    "In this notebook, we implement **Bayes Search with cross-validation** from scratch using **a custom class `MyBayesSearchCV`**. We then compare the performance of these implementations with **scikit-learn**'s `BayesSearchCV`, **optuna**'s  and **hyperopt**'s models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae7e75",
   "metadata": {},
   "source": [
    "### ⚙️ Importing Libraries & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dfd5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from optuna.trial import Trial\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import (\n",
    "    BaseCrossValidator,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d14d1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.width\", 150)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f36ff",
   "metadata": {},
   "source": [
    "### 📥 Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b607b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c28f6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56902d14",
   "metadata": {},
   "source": [
    "### 🧠 Implementing Custom Model Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "19b2063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBayesSearchCV:\n",
    "    \"\"\"Bayesian hyperparameter optimization with cross-validation.\n",
    "\n",
    "    This class combines random sampling with Bayesian optimization using a\n",
    "    Tree-structured Parzen Estimator (TPE) approach to efficiently search\n",
    "    for the best hyperparameters of an estimator. The search is guided by\n",
    "    Expected Improvement, focusing on maximizing cross-validated accuracy.\n",
    "\n",
    "    Attributes:\n",
    "        estimator (BaseEstimator): The machine learning estimator to optimize.\n",
    "        search_spaces (dict[str, Real | Integer]): Hyperparameter search distributions.\n",
    "        n_iter (int): Total number of hyperparameter configurations to evaluate.\n",
    "        cv (int | BaseCrossValidator): Cross-validation splitting strategy.\n",
    "        random_state (int | None): Seed for reproducible randomness.\n",
    "        best_estimator_ (BaseEstimator | None): Fitted estimator with best found params.\n",
    "        best_params_ (dict[str, Any] | None): Best hyperparameters found during search.\n",
    "        best_score_ (float): Best cross-validation accuracy score obtained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: BaseEstimator,\n",
    "        search_spaces: dict[str, Real | Integer],\n",
    "        n_iter: int,\n",
    "        cv: int | BaseCrossValidator = 5,\n",
    "        random_state: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the Bayesian hyperparameter search.\n",
    "\n",
    "        Args:\n",
    "            estimator (BaseEstimator): Estimator object to optimize.\n",
    "            search_spaces (dict[str, Real | Integer]): Dictionary mapping hyperparameter\n",
    "                names to their search distributions (e.g. from scipy.stats).\n",
    "            n_iter (int): Total number of parameter settings to sample.\n",
    "            cv (int | BaseCrossValidator, optional): Cross-validation strategy,\n",
    "                either an integer (number of folds) or a cross-validator instance.\n",
    "                Defaults to 5.\n",
    "            random_state (int | None, optional): Random seed for reproducibility.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "        self.search_spaces = search_spaces\n",
    "        self.n_iter = n_iter\n",
    "        self.n_candidates: int = 30\n",
    "        self.gamma: float = 0.2\n",
    "\n",
    "        if isinstance(cv, int):\n",
    "            self.cv = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "        else:\n",
    "            self.cv = cv\n",
    "\n",
    "        self._rng = np.random.RandomState(random_state)\n",
    "        self.best_estimator_: BaseEstimator | None = None\n",
    "        self.best_params_: dict[str, Any] | None = None\n",
    "        self.best_score_: float = float(\"-inf\")\n",
    "        self._X_: list[list[float]] = []\n",
    "        self._y_: list[float] = []\n",
    "\n",
    "        self._dimensions = list(self.search_spaces.values())\n",
    "        self._param_names = list(self.search_spaces.keys())\n",
    "\n",
    "    def _sample_params(self) -> tuple[dict[str, Any], list[float]]:\n",
    "        \"\"\"Sample a set of hyperparameters from the search space.\n",
    "\n",
    "        Returns:\n",
    "            tuple[dict[str, Any], list[float]]: A dictionary of sampled parameters and\n",
    "                the corresponding parameter values as a list.\n",
    "        \"\"\"\n",
    "        sampled = [dim.rvs(random_state=self._rng)[0] for dim in self._dimensions]\n",
    "        params = dict(zip(self._param_names, sampled, strict=False))\n",
    "        return params, sampled\n",
    "\n",
    "    def _evaluate(\n",
    "        self, params: dict[str, Any], X: NDArray[np.float64], y: NDArray[np.int64]\n",
    "    ) -> float:\n",
    "        \"\"\"Evaluate model performance with given parameters using cross-validation.\n",
    "\n",
    "        Args:\n",
    "            params (dict[str, Any]): Parameters to evaluate.\n",
    "            X (NDArray[np.float64]): Feature matrix.\n",
    "            y (NDArray[np.int64]): Target labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Mean accuracy across all CV folds.\n",
    "        \"\"\"\n",
    "        estimator = clone(self.estimator)\n",
    "        estimator.set_params(**params)\n",
    "\n",
    "        scores = []\n",
    "        for train_idx, valid_idx in self.cv.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[valid_idx]\n",
    "            y_train, y_test = y[train_idx], y[valid_idx]\n",
    "\n",
    "            estimator.fit(X_train, y_train)\n",
    "            y_pred = estimator.predict(X_test)\n",
    "\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    def _safe_gaussian_kde(self, data: NDArray[np.float64]) -> callable:\n",
    "        \"\"\"Safely create a Gaussian KDE or fallback to uniform if data is degenerate.\n",
    "\n",
    "        Args:\n",
    "            data (NDArray[np.float64]): 1D array of data points.\n",
    "\n",
    "        Returns:\n",
    "            callable: KDE function or constant function returning 1.\n",
    "        \"\"\"\n",
    "        if data.size < 2 or np.all(data == data[0]):\n",
    "            return lambda x: [1.0]\n",
    "        else:\n",
    "            return gaussian_kde(data)\n",
    "\n",
    "    def _tpe_suggest(\n",
    "        self, X_sampled: NDArray[np.float64], y_sampled: NDArray[np.float64]\n",
    "    ) -> tuple[dict[str, Any], list[float]]:\n",
    "        \"\"\"Suggest the next hyperparameter set to evaluate using TPE.\n",
    "\n",
    "        The method fits KDEs to the best and the rest of the observed samples,\n",
    "        then samples candidates and selects the one maximizing the ratio\n",
    "        of densities (Expected Improvement).\n",
    "\n",
    "        Args:\n",
    "            X_sampled (NDArray[np.float64]): Previously sampled parameter vectors.\n",
    "            y_sampled (NDArray[np.float64]): Corresponding scores for sampled params.\n",
    "\n",
    "        Returns:\n",
    "            tuple[dict[str, Any], list[float]]: Suggested hyperparameters.\n",
    "        \"\"\"\n",
    "        n_best = max(2, int(self.gamma * len(y_sampled)))\n",
    "        indices_sorted = np.argsort(y_sampled)[::-1]\n",
    "        best_indices = indices_sorted[:n_best]\n",
    "        rest_indices = indices_sorted[n_best:]\n",
    "\n",
    "        X_best = X_sampled[best_indices]\n",
    "        X_rest = X_sampled[rest_indices]\n",
    "\n",
    "        kde_best = [\n",
    "            self._safe_gaussian_kde(X_best[:, i]) for i in range(X_best.shape[1])\n",
    "        ]\n",
    "        kde_rest = [\n",
    "            self._safe_gaussian_kde(X_rest[:, i]) for i in range(X_rest.shape[1])\n",
    "        ]\n",
    "\n",
    "        candidates_params = []\n",
    "        candidates_x = []\n",
    "        for _ in range(self.n_candidates):\n",
    "            params, x = self._sample_params()\n",
    "            candidates_params.append(params)\n",
    "            candidates_x.append(x)\n",
    "\n",
    "        scores = []\n",
    "        for x in candidates_x:\n",
    "            p_best = np.prod([kde_best[i](x[i])[0] for i in range(len(x))])\n",
    "            p_rest = np.prod([kde_rest[i](x[i])[0] for i in range(len(x))])\n",
    "            score = p_best / p_rest if p_rest > 0 else np.inf\n",
    "            scores.append(score)\n",
    "\n",
    "        best_idx = int(np.argmax(scores))\n",
    "        return candidates_params[best_idx], candidates_x[best_idx]\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.int64 | np.float64]) -> None:\n",
    "        \"\"\"Perform hyperparameter search with Bayesian optimization.\n",
    "\n",
    "        Args:\n",
    "            X (NDArray[np.float64]): Feature matrix of shape (n_samples, n_features).\n",
    "            y (NDArray[np.int64 | np.float64]): Target vector of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        init_points = min(10, self.n_iter)\n",
    "        for _ in range(init_points):\n",
    "            params, x = self._sample_params()\n",
    "            score = self._evaluate(params, X, y)\n",
    "            self._X_.append(x)\n",
    "            self._y_.append(score)\n",
    "\n",
    "            if self.best_score_ < score:\n",
    "                self.best_score_ = score\n",
    "                self.best_params_ = params\n",
    "\n",
    "        for _ in range(self.n_iter - init_points):\n",
    "            X_train = np.array(self._X_)\n",
    "            y_train = np.array(self._y_)\n",
    "\n",
    "            best_params, best_x = self._tpe_suggest(X_train, y_train)\n",
    "\n",
    "            score = self._evaluate(best_params, X, y)\n",
    "            self._X_.append(best_x)\n",
    "            self._y_.append(score)\n",
    "\n",
    "            if self.best_score_ < score:\n",
    "                self.best_score_ = score\n",
    "                self.best_params_ = best_params\n",
    "\n",
    "        self.best_estimator_ = clone(self.estimator)\n",
    "        self.best_estimator_.set_params(**self.best_params_)\n",
    "        self.best_estimator_.fit(X, y)\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.int64 | np.float64]:\n",
    "        \"\"\"Predict target values using the best found estimator.\n",
    "\n",
    "        Args:\n",
    "            X (NDArray[np.float64]): Feature matrix for prediction.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If fit() has not been called yet.\n",
    "\n",
    "        Returns:\n",
    "            NDArray[np.int64 | np.float64]: Predicted target values.\n",
    "        \"\"\"\n",
    "        if self.best_estimator_ is None:\n",
    "            raise ValueError(\"fit() must be called before predict()\")\n",
    "        return self.best_estimator_.predict(X)\n",
    "\n",
    "    def score(self, X: NDArray[np.float64], y: NDArray[np.int64 | np.float64]) -> float:\n",
    "        \"\"\"Compute accuracy of the best estimator on the given data.\n",
    "\n",
    "        Args:\n",
    "            X (NDArray[np.float64]): Feature matrix.\n",
    "            y (NDArray[np.int64]): True labels.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If fit() has not been called yet.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy score.\n",
    "        \"\"\"\n",
    "        if self.best_estimator_ is None:\n",
    "            raise ValueError(\"fit() must be called before score()\")\n",
    "        return self.best_estimator_.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37469d",
   "metadata": {},
   "source": [
    "### 🏋️‍♂️ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f1e200ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 19:50:47,875] A new study created in memory with name: no-name-bb0d4a86-dfce-43d8-ad5d-326815e0be8f\n",
      "[I 2025-07-30 19:50:47,890] Trial 0 finished with value: 0.4333333333333333 and parameters: {'C': 1.091625102864551e-05, 'gamma': 0.00045798892646589477, 'degree': 4}. Best is trial 0 with value: 0.4333333333333333.\n",
      "[I 2025-07-30 19:50:47,900] Trial 1 finished with value: 0.4749999999999999 and parameters: {'C': 0.03568114384119702, 'gamma': 0.005283939818976039, 'degree': 3}. Best is trial 1 with value: 0.4749999999999999.\n",
      "[I 2025-07-30 19:50:47,911] Trial 2 finished with value: 0.4333333333333333 and parameters: {'C': 26.133107914438046, 'gamma': 0.002225465625781207, 'degree': 4}. Best is trial 1 with value: 0.4749999999999999.\n",
      "[I 2025-07-30 19:50:47,922] Trial 3 finished with value: 0.9666666666666667 and parameters: {'C': 19.796941324709028, 'gamma': 0.015756113937268366, 'degree': 3}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,933] Trial 4 finished with value: 0.8916666666666666 and parameters: {'C': 0.00420058709948238, 'gamma': 0.04511071531971357, 'degree': 5}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,943] Trial 5 finished with value: 0.4166666666666667 and parameters: {'C': 0.12371485075348039, 'gamma': 0.008839693370755249, 'degree': 5}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,952] Trial 6 finished with value: 0.8583333333333334 and parameters: {'C': 0.023492026274074432, 'gamma': 0.07031350070091441, 'degree': 2}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,961] Trial 7 finished with value: 0.9499999999999998 and parameters: {'C': 1.1731942031622122, 'gamma': 0.04504594029580549, 'degree': 5}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,971] Trial 8 finished with value: 0.525 and parameters: {'C': 1.3750164711333855e-05, 'gamma': 0.0002799271745135734, 'degree': 2}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:47,978] Trial 9 finished with value: 0.4166666666666667 and parameters: {'C': 2.6119730073568605e-05, 'gamma': 0.00115694977763125, 'degree': 5}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:48,048] Trial 10 finished with value: 0.9583333333333334 and parameters: {'C': 86.76331818262594, 'gamma': 0.015551034291688061, 'degree': 3}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:48,072] Trial 11 finished with value: 0.9583333333333334 and parameters: {'C': 83.7210587194794, 'gamma': 0.014594980377134853, 'degree': 3}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:48,099] Trial 12 finished with value: 0.9500000000000001 and parameters: {'C': 4.609640738907252, 'gamma': 0.01385922194105516, 'degree': 3}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2025-07-30 19:50:48,122] Trial 13 finished with value: 0.975 and parameters: {'C': 3.6965612384165585, 'gamma': 0.023088646768288312, 'degree': 3}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,148] Trial 14 finished with value: 0.525 and parameters: {'C': 1.8408188312400697, 'gamma': 0.00011493666163316935, 'degree': 2}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,173] Trial 15 finished with value: 0.9666666666666667 and parameters: {'C': 7.856594207099872, 'gamma': 0.02936634554896522, 'degree': 4}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,198] Trial 16 finished with value: 0.4749999999999999 and parameters: {'C': 0.21375007349776992, 'gamma': 0.00337086639190009, 'degree': 3}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,226] Trial 17 finished with value: 0.4333333333333333 and parameters: {'C': 0.0002203992852737566, 'gamma': 0.025594965069690105, 'degree': 4}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,248] Trial 18 finished with value: 0.9666666666666667 and parameters: {'C': 0.7675422215296722, 'gamma': 0.0995759423215882, 'degree': 2}. Best is trial 13 with value: 0.975.\n",
      "[I 2025-07-30 19:50:48,265] Trial 19 finished with value: 0.8999999999999999 and parameters: {'C': 14.740906045136452, 'gamma': 0.006958773067180578, 'degree': 3}. Best is trial 13 with value: 0.975.\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = SVC(kernel=\"poly\")\n",
    "\n",
    "# Hyperparameter spaces\n",
    "search_spaces = {\n",
    "    \"C\": Real(1e-2, 100.0, prior=\"log-uniform\"),\n",
    "    \"gamma\": Real(1e-4, 1e-1, prior=\"log-uniform\"),\n",
    "    \"degree\": Integer(2, 5),\n",
    "}\n",
    "\n",
    "\n",
    "def objective(trial: Trial) -> float:\n",
    "    \"\"\"Objective function for Optuna study to optimize hyperparameters of SVC.\n",
    "\n",
    "    Parameters:\n",
    "        trial (Trial): A single trial object that suggests hyperparameter values.\n",
    "\n",
    "    Returns:\n",
    "        float: Cross-validated accuracy score (mean of folds).\n",
    "    \"\"\"\n",
    "    C = trial.suggest_float(\"C\", 1e-5, 100.0, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1e-4, 1e-1, log=True)\n",
    "    degree = trial.suggest_int(\"degree\", 2, 5)\n",
    "\n",
    "    model = SVC(C=C, gamma=gamma, degree=degree, kernel=\"poly\")\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Optuna\n",
    "optuna_bs = optuna.create_study(direction=\"maximize\")\n",
    "optuna_bs.optimize(objective, n_trials=20, timeout=60)\n",
    "\n",
    "# Scikit-learn BayesSearchCV\n",
    "sklearn_bs = BayesSearchCV(\n",
    "    estimator=model, search_spaces=search_spaces, n_iter=20, random_state=42\n",
    ")\n",
    "sklearn_bs.fit(X_train, y_train)\n",
    "\n",
    "# My BayesSearchCV\n",
    "my_bs = MyBayesSearchCV(\n",
    "    estimator=model, search_spaces=search_spaces, n_iter=20, random_state=42\n",
    ")\n",
    "my_bs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37549ace",
   "metadata": {},
   "source": [
    "### 📊 Comparing Algorithm Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d216f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_model = clone(model)\n",
    "optuna_model.set_params(**optuna_bs.best_params)\n",
    "optuna_model.fit(X_train, y_train)\n",
    "optuna_pred = optuna_model.predict(X_test)\n",
    "optuna_test_score = accuracy_score(optuna_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4baa28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>degree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>best_cv_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optuna</th>\n",
       "      <td>3.696561</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom</th>\n",
       "      <td>0.029349</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.065993</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  C  degree     gamma  best_cv_score  test_score\n",
       "sklearn  100.000000     2.0  0.015539       0.958333         1.0\n",
       "optuna     3.696561     3.0  0.023089       0.975000         1.0\n",
       "custom     0.029349     4.0  0.065993       0.966667         1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_search_algorithms = {\n",
    "    \"sklearn\": {\n",
    "        **sklearn_bs.best_params_,\n",
    "        \"best_cv_score\": sklearn_bs.best_score_,\n",
    "        \"test_score\": sklearn_bs.score(X_test, y_test),\n",
    "    },\n",
    "    \"optuna\": {\n",
    "        **optuna_bs.best_params,\n",
    "        \"best_cv_score\": optuna_bs.best_value,\n",
    "        \"test_score\": optuna_test_score,\n",
    "    },\n",
    "    \"custom\": {\n",
    "        **my_bs.best_params_,\n",
    "        \"best_cv_score\": my_bs.best_score_,\n",
    "        \"test_score\": my_bs.score(X_test, y_test),\n",
    "    },\n",
    "}\n",
    "\n",
    "pd.DataFrame(bayes_search_algorithms).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    \"\"\"Objective function for Optuna study to optimize hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "        trial (Trial): A single trial object that suggests hyperparameter values.\n",
    "\n",
    "    Returns:\n",
    "        float: Cross-validated accuracy score (mean of folds).\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = trial.suggest_categorical('model', ['xgb', 'rf', 'svm'])\n",
    "\n",
    "    if model_name == 'xgb':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('xgb_n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('xgb_max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('xgb_subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.5, 1.0),\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    elif model_name == 'svm':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('svm_C', 1e-3, 100, log=True),\n",
    "            'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'rbf', 'poly']),\n",
    "            'gamma': 'scale',\n",
    "        'random_state': 42\n",
    "        }\n",
    "        if params['kernel'] == 'poly':\n",
    "            params['degree'] = trial.suggest_int('svm_degree', 2, 5)\n",
    "        model = SVC(**params)\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('rf_n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('rf_max_depth', 3, 30),\n",
    "            'max_features': trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None]),\n",
    "            'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 10),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "estimators = {\n",
    "    \"rf\": RandomForestClassifier,\n",
    "    \"xgb\": XGBClassifier,\n",
    "    \"svm\"; SVC,\n",
    "}\n",
    "\n",
    "\n",
    "optuna_bs = optuna.create_study(direction=\"maximize\")\n",
    "optuna_bs.optimize(objective, n_trials=20, timeout=60)\n",
    "\n",
    "params = study.best_params\n",
    "model_name = params.pop('model')\n",
    "\n",
    "model = estimators[model_name](**params)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = optuna_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-template-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
