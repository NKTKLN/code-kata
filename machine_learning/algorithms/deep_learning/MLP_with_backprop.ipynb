{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b333389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import Generator as RandomGenerator\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17bc2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fae8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    \"\"\"Инициализатор весов нейронной сети по имени стратегии и распределения.\n",
    "\n",
    "    Предоставляет единый интерфейс для генерации матриц весов на основе строкового имени\n",
    "    инициализатора в формате <strategy>_<distribution>. Поддерживает стратегии random,\n",
    "    xavier и kaiming, а также распределения normal и uniform.\n",
    "\n",
    "    Attributes:\n",
    "        _rng (RandomGenerator): Генератор случайных чисел.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng: RandomGenerator | None = None):\n",
    "        \"\"\"Инициализирует объект инициализации весов.\n",
    "\n",
    "        Args:\n",
    "            rng (RandomGenerator | None): Генератор случайных чисел. Если не задан,\n",
    "                используется генератор по умолчанию np.random.default_rng().\n",
    "        \"\"\"\n",
    "        self._rng: RandomGenerator = rng or np.random.default_rng()\n",
    "\n",
    "    def _normal(self, var: float, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Генерирует матрицу весов из нормального распределения с заданным масштабом.\n",
    "\n",
    "        Args:\n",
    "            var (float): Параметр масштаба распределения, используемый при генерации.\n",
    "            shape (tuple[int, int]): Форма генерируемой матрицы весов.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Матрица весов указанной формы.\n",
    "        \"\"\"\n",
    "        return self._rng.normal(0, var, shape)\n",
    "\n",
    "    def _uniform(self, var: float, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Генерирует матрицу весов из равномерного распределения с заданным масштабом.\n",
    "\n",
    "        Масштаб преобразуется к границам равномерного распределения так, чтобы\n",
    "        соответствовать выбранной стратегии инициализации.\n",
    "\n",
    "        Args:\n",
    "            var (float): Базовый параметр масштаба.\n",
    "            shape (tuple[int, int]): Форма генерируемой матрицы весов.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Матрица весов указанной формы.\n",
    "        \"\"\"\n",
    "        bound = np.sqrt(3 * var)\n",
    "        return self._rng.uniform(-bound, bound, shape)\n",
    "\n",
    "    def _parse_name(self, name: str) -> tuple[str, str]:\n",
    "        \"\"\"Разбирает имя инициализатора на стратегию и распределение.\n",
    "\n",
    "        Ожидаемый формат имени: <strategy>_<distribution>, например \"xavier_uniform\".\n",
    "\n",
    "        Args:\n",
    "            name (str): Имя инициализатора в строковом формате.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Если строка не соответствует формату <strategy>_<distribution>.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, str]: Кортеж из стратегии и распределения.\n",
    "        \"\"\"\n",
    "        parts = name.split(\"_\")\n",
    "\n",
    "        if len(parts) != 2:\n",
    "            raise ValueError(\n",
    "                f\"Invalid format '{name}'. Expected format: <strategy>_<distribution>\"\n",
    "            )\n",
    "\n",
    "        return tuple(parts)\n",
    "\n",
    "    def __call__(self, name: str, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Создаёт матрицу весов согласно выбранной стратегии и распределению.\n",
    "\n",
    "        Поддерживаемые стратегии:\n",
    "            - random: Масштаб 1 / n_in.\n",
    "            - xavier: Масштаб 2 / (n_in + n_out).\n",
    "            - kaiming: Масштаб 2 / n_in.\n",
    "\n",
    "        Поддерживаемые распределения:\n",
    "            - normal: Нормальное распределение.\n",
    "            - uniform: Равномерное распределение.\n",
    "\n",
    "        Args:\n",
    "            name (str): Имя инициализатора в формате <strategy>_<distribution>.\n",
    "            shape (tuple[int, int]): Форма матрицы весов (n_in, n_out).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Если указана неподдерживаемая стратегия инициализации.\n",
    "            ValueError: Если указано неподдерживаемое распределение.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Матрица весов указанной формы.\n",
    "        \"\"\"\n",
    "        fan_in, fan_out = shape\n",
    "        strategy, dist = self._parse_name(name)\n",
    "\n",
    "        match strategy:\n",
    "            case \"random\":\n",
    "                var = 1 / fan_in\n",
    "            case \"xavier\":\n",
    "                var = 2 / (fan_in + fan_out)\n",
    "            case \"kaiming\":\n",
    "                var = 2 / fan_in\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "        match dist:\n",
    "            case \"normal\":\n",
    "                return self._normal(var, shape)\n",
    "            case \"uniform\":\n",
    "                return self._uniform(var, shape)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown distribution: {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdac5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"Базовый абстрактный класс слоя нейронной сети.\n",
    "\n",
    "    Определяет общий интерфейс для всех слоёв модели, включая методы\n",
    "    прямого и обратного распространения, шаг обновления параметров,\n",
    "    а также переключение между режимами обучения и инференса.\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        weight_init (str): Имя стратегии инициализации весов\n",
    "            в формате <strategy>_<distribution>.\n",
    "        is_train (bool): Флаг режима работы слоя.\n",
    "            True — режим обучения, False — режим инференса.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"random_uniform\"):\n",
    "        \"\"\"Инициализирует базовый слой.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): Имя стратегии инициализации весов,\n",
    "                используемое при создании параметров слоя.\n",
    "        \"\"\"\n",
    "        self.weight_init = weight_init\n",
    "        self.is_train = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Выполняет прямой проход слоя.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входные данные слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Результат преобразования входных данных.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        \"\"\"Выполняет обратное распространение градиента через слой.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): Градиент функции потерь по выходу слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу слоя.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, _lr: float) -> None:\n",
    "        \"\"\"Выполняет шаг обновления параметров слоя.\n",
    "\n",
    "        Базовая реализация предназначена для слоёв без обучаемых параметров.\n",
    "\n",
    "        Args:\n",
    "            _lr (float): Скорость обучения.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Переводит слой в режим обучения.\"\"\"\n",
    "        self.is_train = True\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        \"\"\"Переводит слой в режим инференса.\"\"\"\n",
    "        self.is_train = False\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"Полносвязный линейный слой нейронной сети.\n",
    "\n",
    "    Выполняет аффинное преобразование входных данных по формуле:\n",
    "        z = W @ x + b\n",
    "\n",
    "    Сохраняет входные данные для последующего вычисления градиентов\n",
    "    на этапе обратного распространения ошибки.\n",
    "\n",
    "    Attributes:\n",
    "        in_dim (int): Число входных признаков.\n",
    "        out_dim (int): Число выходных признаков.\n",
    "        w (np.ndarray): Матрица весов формы (out_dim, in_dim).\n",
    "        b (np.ndarray): Вектор смещений формы (out_dim, 1).\n",
    "        dw (np.ndarray | None): Градиент функции потерь по матрице весов.\n",
    "        db (np.ndarray | None): Градиент функции потерь по вектору смещений.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, weight_init: str = \"previous\"\n",
    "    ) -> None:\n",
    "        \"\"\"Инициализирует линейный слой с заданной размерностью.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Число входных признаков.\n",
    "            out_dim (int): Число выходных признаков.\n",
    "            weight_init (str): Имя стратегии инициализации весов.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.w: np.ndarray\n",
    "        self.b = np.zeros((out_dim, 1))\n",
    "\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Выполняет прямой проход линейного слоя.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входной тензор формы (in_dim, batch_size).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Выходной тензор формы (out_dim, batch_size).\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.z = self.w @ x + self.b\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Вычисляет градиенты по параметрам и по входу слоя.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): Градиент функции потерь по выходу слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу слоя.\n",
    "        \"\"\"\n",
    "        if self.x is None:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() before forward(). \"\n",
    "                \"Linear layer input is not cached.\"\n",
    "            )\n",
    "\n",
    "        self.dw = grad @ self.x.T\n",
    "        self.db = np.sum(grad, axis=1, keepdims=True)\n",
    "\n",
    "        grad_x = self.w.T @ grad\n",
    "        return grad_x\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Обновляет параметры слоя с использованием вычисленных градиентов.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Скорость обучения.\n",
    "        \"\"\"\n",
    "        if self.db is None or self.dw is None:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot update parameters before gradients are computed. \"\n",
    "                \"Call backward() before step().\"\n",
    "            )\n",
    "\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"Сигмоидная функция активации.\n",
    "\n",
    "    Преобразует входные значения в диапазон (0, 1) по формуле:\n",
    "        σ(z) = 1 / (1 + exp(-z))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"kaiming_uniform\"):\n",
    "        \"\"\"Инициализирует слой сигмоидной активации.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): Имя стратегии инициализации,\n",
    "                передаётся для согласованности интерфейса.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Выполняет прямой проход сигмоидной активации.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входной тензор предактивации.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Выходной тензор после применения сигмоиды.\n",
    "        \"\"\"\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        \"\"\"Вычисляет градиент сигмоиды по входу.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): Градиент функции потерь по выходу слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу слоя.\n",
    "        \"\"\"\n",
    "        if self.y is None:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() before forward(). Sigmoid output is not cached.\"\n",
    "            )\n",
    "\n",
    "        return grad * self.y * (1 - self.y)\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    \"\"\"Слой Dropout для регуляризации нейронной сети.\n",
    "\n",
    "    В режиме обучения случайно зануляет элементы входа с вероятностью p\n",
    "    и масштабирует оставшиеся элементы для сохранения математического ожидания.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p: float, weight_init: str = \"previous\"):\n",
    "        \"\"\"Инициализирует слой Dropout.\n",
    "\n",
    "        Args:\n",
    "            p (float): Вероятность зануления элемента.\n",
    "            weight_init (str): Имя стратегии инициализации,\n",
    "                передаётся для согласованности интерфейса.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Применяет Dropout к входным данным в режиме обучения.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входные данные слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Выходные данные после применения маски Dropout.\n",
    "        \"\"\"\n",
    "        if not self.is_train or self.p == 0:\n",
    "            return x\n",
    "\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p).astype(x.dtype)\n",
    "        self.mask /= 1.0 - self.p\n",
    "\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        \"\"\"Применяет маску Dropout к градиенту на обратном проходе.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): Градиент функции потерь по выходу слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу слоя.\n",
    "        \"\"\"\n",
    "        if self.mask is None:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() before forward() in training mode. \"\n",
    "                \"Dropout mask is not initialized.\"\n",
    "            )\n",
    "\n",
    "        return grad * self.mask\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    \"\"\"Функция активации ReLU (Rectified Linear Unit).\n",
    "\n",
    "    Выполняет преобразование:\n",
    "        f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"xavier_uniform\"):\n",
    "        \"\"\"Инициализирует слой ReLU.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): Имя стратегии инициализации,\n",
    "                передаётся для согласованности интерфейса.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Применяет функцию ReLU к входным данным.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входной тензор.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Выходной тензор после применения ReLU.\n",
    "        \"\"\"\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad: np.ndarray):\n",
    "        \"\"\"Вычисляет градиент ReLU по входу.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): Градиент функции потерь по выходу слоя.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу слоя.\n",
    "        \"\"\"\n",
    "        if self.mask is None:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() before forward(). \"\n",
    "                \"ReLU activation mask is not initialized.\"\n",
    "            )\n",
    "\n",
    "        return grad * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c645c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Последовательная нейронная сеть, объединяющая набор слоёв.\n",
    "\n",
    "    Управляет инициализацией весов линейных слоёв, выполнением прямого прохода,\n",
    "    обратного распространения градиента и обновлением параметров. Также поддерживает\n",
    "    переключение всех слоёв между режимами обучения и инференса.\n",
    "\n",
    "    Инициализация весов выполняется с учётом стратегии, указанной в слое. Если для слоя\n",
    "    задано значение \"previous\", используется стратегия инициализации, унаследованная\n",
    "    от последнего встреченного слоя, который явно указал стратегию.\n",
    "\n",
    "    Attributes:\n",
    "        layers (list[Layer]): Список слоёв в порядке применения.\n",
    "        _rng (np.random.Generator): Генератор случайных чисел для воспроизводимости.\n",
    "        w_init (WeightInitializer): Инициализатор весов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: list[Layer], random_state: int | None = None) -> None:\n",
    "        \"\"\"Инициализирует нейронную сеть и выполняет инициализацию весов.\n",
    "\n",
    "        Args:\n",
    "            layers (list[Layer]): Слои сети в порядке выполнения.\n",
    "            random_state (int | None): Начальное состояние генератора случайных чисел.\n",
    "                Если не задано, используется недетерминированная инициализация.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "\n",
    "        self.w_init = WeightInitializer(self._rng)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Инициализирует веса линейных слоёв в соответствии с выбранными стратегиями.\n",
    "\n",
    "        Проходит по слоям в обратном порядке, чтобы определить стратегию инициализации\n",
    "        для линейных слоёв на основе параметра weight_init. Если слой имеет значение\n",
    "        \"previous\", то используется ранее определённая стратегия инициализации.\n",
    "        \"\"\"\n",
    "        prev_init = \"random_uniform\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            if isinstance(layer, Linear):\n",
    "                init_name = prev_init\n",
    "                if layer.weight_init != \"previous\":\n",
    "                    init_name = layer.weight_init\n",
    "                layer.w = self.w_init(init_name, (layer.out_dim, layer.in_dim))\n",
    "\n",
    "            layer_init = layer.weight_init\n",
    "            if layer_init != \"previous\":\n",
    "                prev_init = layer_init\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Выполняет прямой проход через все слои сети.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Входные данные сети.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Выход сети после последовательного применения всех слоёв.\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Выполняет обратное распространение градиента через все слои сети.\n",
    "\n",
    "        Args:\n",
    "            grad_out (np.ndarray): Градиент функции потерь по выходу сети.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Градиент функции потерь по входу сети.\n",
    "        \"\"\"\n",
    "        grad = grad_out\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Обновляет параметры всех слоёв сети с заданной скоростью обучения.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Скорость обучения.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(lr)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Переводит все слои сети в режим обучения.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        \"\"\"Переводит все слои сети в режим инференса.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be0076b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 30\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    [\n",
    "        Linear(n_features, 64),\n",
    "        Dropout(0.3),\n",
    "        ReLU(),\n",
    "        Linear(64, 32),\n",
    "        Dropout(0.2),\n",
    "        ReLU(),\n",
    "        Linear(32, 1),\n",
    "        Sigmoid(),\n",
    "    ],\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "204c50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train log_loss: 0.1852 | train acc: 0.9275 | test log_loss: 0.2058 | test acc: 0.9123\n",
      "Epoch  10 | train log_loss: 0.0784 | train acc: 0.9758 | test log_loss: 0.1033 | test acc: 0.9561\n",
      "Epoch  20 | train log_loss: 0.0615 | train acc: 0.9846 | test log_loss: 0.0998 | test acc: 0.9561\n",
      "Epoch  30 | train log_loss: 0.0525 | train acc: 0.9868 | test log_loss: 0.0969 | test acc: 0.9561\n",
      "Epoch  40 | train log_loss: 0.051 | train acc: 0.989 | test log_loss: 0.0921 | test acc: 0.9561\n",
      "Epoch  50 | train log_loss: 0.0485 | train acc: 0.989 | test log_loss: 0.0941 | test acc: 0.9561\n",
      "Epoch  60 | train log_loss: 0.0458 | train acc: 0.989 | test log_loss: 0.0922 | test acc: 0.9561\n",
      "Epoch  70 | train log_loss: 0.0439 | train acc: 0.9912 | test log_loss: 0.0947 | test acc: 0.9561\n",
      "Epoch  80 | train log_loss: 0.0427 | train acc: 0.9912 | test log_loss: 0.097 | test acc: 0.9561\n",
      "Epoch  90 | train log_loss: 0.0417 | train acc: 0.9912 | test log_loss: 0.0966 | test acc: 0.9561\n",
      "Epoch 100 | train log_loss: 0.0416 | train acc: 0.9912 | test log_loss: 0.1007 | test acc: 0.9561\n",
      "Epoch 110 | train log_loss: 0.0407 | train acc: 0.9934 | test log_loss: 0.0977 | test acc: 0.9561\n",
      "Epoch 120 | train log_loss: 0.0401 | train acc: 0.9934 | test log_loss: 0.0994 | test acc: 0.9561\n",
      "Epoch 130 | train log_loss: 0.0392 | train acc: 0.9934 | test log_loss: 0.0996 | test acc: 0.9561\n",
      "Epoch 140 | train log_loss: 0.0386 | train acc: 0.9934 | test log_loss: 0.1025 | test acc: 0.9561\n",
      "Epoch 150 | train log_loss: 0.0378 | train acc: 0.9956 | test log_loss: 0.1047 | test acc: 0.9561\n",
      "Epoch 160 | train log_loss: 0.0377 | train acc: 0.9956 | test log_loss: 0.1015 | test acc: 0.9561\n",
      "Epoch 170 | train log_loss: 0.0382 | train acc: 0.9956 | test log_loss: 0.1003 | test acc: 0.9561\n",
      "Epoch 180 | train log_loss: 0.0383 | train acc: 0.9956 | test log_loss: 0.1029 | test acc: 0.9561\n",
      "Epoch 190 | train log_loss: 0.0372 | train acc: 0.9956 | test log_loss: 0.1088 | test acc: 0.9561\n",
      "Epoch 200 | train log_loss: 0.0365 | train acc: 0.9956 | test log_loss: 0.1113 | test acc: 0.9561\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "N = X_train.shape[1]\n",
    "\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    nn.train()\n",
    "\n",
    "    idx = np.random.permutation(N)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch_idx = idx[start : start + batch_size]\n",
    "        x = X_train[:, batch_idx]\n",
    "        yb = y_train[:, batch_idx]\n",
    "        B = x.shape[1]\n",
    "\n",
    "        y_hat = nn.forward(x)\n",
    "        grad = (y_hat - yb) / B\n",
    "        nn.backward(grad)\n",
    "        nn.step(learning_rate)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        nn.eval()\n",
    "\n",
    "        y_train_pred = nn.forward(X_train).ravel()\n",
    "        y_test_pred = nn.forward(X_test).ravel()\n",
    "\n",
    "        y_train_true = y_train.ravel()\n",
    "        y_test_true = y_test.ravel()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:>3}\",\n",
    "            \"| train log_loss:\",\n",
    "            round(log_loss(y_train_true, y_train_pred), 4),\n",
    "            \"| train acc:\",\n",
    "            round(accuracy_score(y_train_true, y_train_pred > 0.5), 4),\n",
    "            \"| test log_loss:\",\n",
    "            round(log_loss(y_test_true, y_test_pred), 4),\n",
    "            \"| test acc:\",\n",
    "            round(accuracy_score(y_test_true, y_test_pred > 0.5), 4),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628c723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1cef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2e09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
