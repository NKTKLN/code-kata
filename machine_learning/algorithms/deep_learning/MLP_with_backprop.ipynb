{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "17bc2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1da10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    \"\"\"Функция активации сигмоида.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7a458d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 30\n",
    "n_hidden = 32\n",
    "n_out = 1\n",
    "\n",
    "w1 = np.random.randn(n_hidden, n_in) * np.sqrt(1.0 / n_in)\n",
    "b1 = np.zeros((n_hidden, 1))\n",
    "\n",
    "w2 = np.random.randn(n_out, n_hidden) * np.sqrt(1.0 / n_hidden)\n",
    "b2 = np.zeros((n_out, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "204c50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train log_loss: 0.0343 | train acc: 0.9912 | test log_loss: 0.0789 | test acc: 0.9561\n",
      "Epoch  10 | train log_loss: 0.0341 | train acc: 0.9912 | test log_loss: 0.0781 | test acc: 0.9649\n",
      "Epoch  20 | train log_loss: 0.0338 | train acc: 0.9912 | test log_loss: 0.0799 | test acc: 0.9474\n",
      "Epoch  30 | train log_loss: 0.0335 | train acc: 0.9912 | test log_loss: 0.0792 | test acc: 0.9561\n",
      "Epoch  40 | train log_loss: 0.0332 | train acc: 0.9912 | test log_loss: 0.0797 | test acc: 0.9474\n",
      "Epoch  50 | train log_loss: 0.0329 | train acc: 0.9912 | test log_loss: 0.0797 | test acc: 0.9474\n",
      "Epoch  60 | train log_loss: 0.0327 | train acc: 0.9912 | test log_loss: 0.0791 | test acc: 0.9649\n",
      "Epoch  70 | train log_loss: 0.0324 | train acc: 0.9912 | test log_loss: 0.0781 | test acc: 0.9649\n",
      "Epoch  80 | train log_loss: 0.0321 | train acc: 0.9912 | test log_loss: 0.0788 | test acc: 0.9649\n",
      "Epoch  90 | train log_loss: 0.0318 | train acc: 0.9912 | test log_loss: 0.0793 | test acc: 0.9649\n",
      "Epoch 100 | train log_loss: 0.0316 | train acc: 0.9912 | test log_loss: 0.0786 | test acc: 0.9649\n",
      "Epoch 110 | train log_loss: 0.0313 | train acc: 0.9912 | test log_loss: 0.0789 | test acc: 0.9649\n",
      "Epoch 120 | train log_loss: 0.031 | train acc: 0.9912 | test log_loss: 0.081 | test acc: 0.9561\n",
      "Epoch 130 | train log_loss: 0.0307 | train acc: 0.9912 | test log_loss: 0.0804 | test acc: 0.9561\n",
      "Epoch 140 | train log_loss: 0.0304 | train acc: 0.9912 | test log_loss: 0.0794 | test acc: 0.9649\n",
      "Epoch 150 | train log_loss: 0.0302 | train acc: 0.9912 | test log_loss: 0.0789 | test acc: 0.9649\n",
      "Epoch 160 | train log_loss: 0.0298 | train acc: 0.9912 | test log_loss: 0.0802 | test acc: 0.9561\n",
      "Epoch 170 | train log_loss: 0.0296 | train acc: 0.9912 | test log_loss: 0.0796 | test acc: 0.9649\n",
      "Epoch 180 | train log_loss: 0.0293 | train acc: 0.9912 | test log_loss: 0.0805 | test acc: 0.9561\n",
      "Epoch 190 | train log_loss: 0.029 | train acc: 0.9912 | test log_loss: 0.0789 | test acc: 0.9649\n",
      "Epoch 200 | train log_loss: 0.0287 | train acc: 0.9912 | test log_loss: 0.081 | test acc: 0.9561\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "N = X_train.shape[1]\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    idx = np.random.permutation(N)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch_idx = idx[start : start + batch_size]\n",
    "        x = X_train[:, batch_idx]\n",
    "        yb = y_train[:, batch_idx]\n",
    "        B = x.shape[1]\n",
    "\n",
    "        z1 = w1 @ x + b1\n",
    "        a1 = sigm(z1)\n",
    "        z2 = w2 @ a1 + b2\n",
    "        y_hat = sigm(z2)\n",
    "\n",
    "        grad_z2 = (y_hat - yb) / B\n",
    "        grad_w2 = grad_z2 @ a1.T\n",
    "        grad_b2 = np.sum(grad_z2, axis=1, keepdims=True)\n",
    "\n",
    "        grad_a1 = w2.T @ grad_z2\n",
    "        grad_z1 = grad_a1 * a1 * (1 - a1)\n",
    "        grad_w1 = grad_z1 @ x.T\n",
    "        grad_b1 = np.sum(grad_z1, axis=1, keepdims=True)\n",
    "\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        w2 -= learning_rate * grad_w2\n",
    "        b2 -= learning_rate * grad_b2\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # train predictions\n",
    "        y_train_pred = sigm(w2 @ sigm(w1 @ X_train + b1) + b2)\n",
    "        y_train_pred = y_train_pred.flatten()\n",
    "        y_train_true = y_train.flatten()\n",
    "\n",
    "        # test predictions\n",
    "        y_test_pred = sigm(w2 @ sigm(w1 @ X_test + b1) + b2)\n",
    "        y_test_pred = y_test_pred.flatten()\n",
    "        y_test_true = y_test.flatten()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:>3}\",\n",
    "            \"| train log_loss:\",\n",
    "            round(log_loss(y_train_true, y_train_pred), 4),\n",
    "            \"| train acc:\",\n",
    "            round(accuracy_score(y_train_true, y_train_pred > 0.5), 4),\n",
    "            \"| test log_loss:\",\n",
    "            round(log_loss(y_test_true, y_test_pred), 4),\n",
    "            \"| test acc:\",\n",
    "            round(accuracy_score(y_test_true, y_test_pred > 0.5), 4),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23cf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628c723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1cef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2e09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
