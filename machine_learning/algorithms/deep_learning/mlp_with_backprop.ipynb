{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe92dc0",
   "metadata": {},
   "source": [
    "# üß™ Custom Multilayer Perceptron\n",
    "\n",
    "In this notebook, we implement linear regression **from scratch**. We then compare the performance of these implementations with **tourch**'s model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71081b09",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Importing Libraries & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b333389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from numpy.random import Generator as RandomGenerator\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5f807983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.width\", 150)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ef433",
   "metadata": {},
   "source": [
    "### üß∞ Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dd343bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_models(\n",
    "    predictions: dict[str, NDArray[np.float64]], y: NDArray[np.float64]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Evaluate multiple classification model predictions on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict[str, NDArray[np.float64]]): Dictionary mapping model names\n",
    "            to predicted target arrays for the dataset.\n",
    "        y (NDArray[np.float64]): True binary target values of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing classification metrics ‚Äî Accuracy, Precision,\n",
    "            Recall, F1-score, and ROC AUC ‚Äî for each model. Columns correspond to\n",
    "            model names, rows correspond to the metric names.\n",
    "    \"\"\"\n",
    "    evaluations = pd.DataFrame(\n",
    "        columns=predictions.keys(),\n",
    "        index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"ROC AUC\"],\n",
    "    )\n",
    "\n",
    "    for name, y_pred in predictions.items():\n",
    "        y_pred_bin = y_pred > 0.5\n",
    "        accuracy = accuracy_score(y, y_pred_bin)\n",
    "        precision = precision_score(y, y_pred_bin)\n",
    "        recall = recall_score(y, y_pred_bin)\n",
    "        f1 = f1_score(y, y_pred_bin)\n",
    "        roc_auc = roc_auc_score(y, y_pred)\n",
    "\n",
    "        evaluations.loc[:, name] = [accuracy, precision, recall, f1, roc_auc]\n",
    "\n",
    "    return evaluations.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b11a3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(\n",
    "    predictions: dict[str, NDArray[np.int64]],\n",
    "    y_true: NDArray[np.int64],\n",
    "    n_cols: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"Plot confusion matrices for multiple classification models.\n",
    "\n",
    "    Each confusion matrix compares the true labels to predictions from one model.\n",
    "    The plots are arranged in a grid layout for easy comparison.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict[str, NDArray[np.int64]]): A dictionary mapping model names to\n",
    "            their predicted labels (1D array of shape (n_samples,)).\n",
    "        y_true (NDArray[np.int64]): True class labels (1D array of shape (n_samples,)).\n",
    "        n_cols (int, optional): Number of columns in the subplot grid layout.\n",
    "            Default is 2.\n",
    "    \"\"\"\n",
    "    n_models = len(predictions)\n",
    "    n_cols = min(n_models, n_cols)\n",
    "    n_rows = ceil(n_models / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, (name, y_pred) in zip(axes, predictions.items(), strict=False):\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "        ax.set_xlabel(\"Predicted Labels\")\n",
    "        ax.set_ylabel(\"True Labels\")\n",
    "        ax.set_title(name)\n",
    "\n",
    "    for i in range(n_models, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86654e59",
   "metadata": {},
   "source": [
    "### üì• Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9424892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.astype(np.float64).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "17bc2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "abf0cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensors\n",
    "X_train_t = torch.tensor(X_train.T, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.T, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test.T, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.T, dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fc27f",
   "metadata": {},
   "source": [
    "### üß† Implementing Custom Model Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1fae8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightInitializer:\n",
    "    \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø–æ –∏–º–µ–Ω–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n",
    "\n",
    "    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏\n",
    "    –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ <strategy>_<distribution>. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ random,\n",
    "    xavier –∏ kaiming, 2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.trans–∞ —Ç–∞–∫–∂–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è normal –∏ uniform.\n",
    "\n",
    "    Attributes:\n",
    "        _rng (RandomGenerator): –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng: RandomGenerator | None = None):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤.\n",
    "\n",
    "        Args:\n",
    "            rng (RandomGenerator | None): –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª. –ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω,\n",
    "                –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é np.random.default_rng().\n",
    "        \"\"\"\n",
    "        self._rng: RandomGenerator = rng or np.random.default_rng()\n",
    "\n",
    "    def _normal(self, var: float, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º.\n",
    "\n",
    "        Args:\n",
    "            var (float): –ü–∞—Ä–∞–º–µ—Ç—Ä –º–∞—Å—à—Ç–∞–±–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "            shape (tuple[int, int]): –§–æ—Ä–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã.\n",
    "        \"\"\"\n",
    "        return self._rng.normal(0, var, shape)\n",
    "\n",
    "    def _uniform(self, var: float, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ –∏–∑ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º.\n",
    "\n",
    "        –ú–∞—Å—à—Ç–∞–± –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –∫ –≥—Ä–∞–Ω–∏—Ü–∞–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∞–∫, —á—Ç–æ–±—ã\n",
    "        —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "        Args:\n",
    "            var (float): –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –º–∞—Å—à—Ç–∞–±–∞.\n",
    "            shape (tuple[int, int]): –§–æ—Ä–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã.\n",
    "        \"\"\"\n",
    "        bound = np.sqrt(3 * var)\n",
    "        return self._rng.uniform(-bound, bound, shape)\n",
    "\n",
    "    def _parse_name(self, name: str) -> tuple[str, str]:\n",
    "        \"\"\"–†–∞–∑–±–∏—Ä–∞–µ—Ç –∏–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n",
    "\n",
    "        –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–º–µ–Ω–∏: <strategy>_<distribution>, –Ω–∞–ø—Ä–∏–º–µ—Ä \"xavier_uniform\".\n",
    "\n",
    "        Args:\n",
    "            name (str): –ò–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç—É <strategy>_<distribution>.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, str]: –ö–æ—Ä—Ç–µ–∂ –∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n",
    "        \"\"\"\n",
    "        parts = name.split(\"_\")\n",
    "\n",
    "        if len(parts) != 2:\n",
    "            raise ValueError(\n",
    "                f\"Invalid format '{name}'. Expected format: <strategy>_<distribution>\"\n",
    "            )\n",
    "\n",
    "        return tuple(parts)\n",
    "\n",
    "    def __call__(self, name: str, shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"–°–æ–∑–¥–∞—ë—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.\n",
    "\n",
    "        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n",
    "            - random: –ú–∞—Å—à—Ç–∞–± 1 / n_in.\n",
    "            - xavier: –ú–∞—Å—à—Ç–∞–± 2 / (n_in + n_out).\n",
    "            - kaiming: –ú–∞—Å—à—Ç–∞–± 2 / n_in.\n",
    "\n",
    "        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:\n",
    "            - normal: –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n",
    "            - uniform: –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n",
    "\n",
    "        Args:\n",
    "            name (str): –ò–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ <strategy>_<distribution>.\n",
    "            shape (tuple[int, int]): –§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ (n_in, n_out).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "            ValueError: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ú–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã.\n",
    "        \"\"\"\n",
    "        fan_in, fan_out = shape\n",
    "        strategy, dist = self._parse_name(name)\n",
    "\n",
    "        match strategy:\n",
    "            case \"random\":\n",
    "                var = 1 / fan_in\n",
    "            case \"xavier\":\n",
    "                var = 2 / (fan_in + fan_out)\n",
    "            case \"kaiming\":\n",
    "                var = 2 / fan_in\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "        match dist:\n",
    "            case \"normal\":\n",
    "                return self._normal(var, shape)\n",
    "            case \"uniform\":\n",
    "                return self._uniform(var, shape)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown distribution: {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1bdac5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"–ë–∞–∑–æ–≤—ã–π –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å —Å–ª–æ—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "\n",
    "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ—ë–≤ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è –º–µ—Ç–æ–¥—ã\n",
    "    –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,\n",
    "    –∞ —Ç–∞–∫–∂–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"random_uniform\") -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–π —Å–ª–æ–π.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤,\n",
    "                –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        self.weight_init = weight_init\n",
    "        self.is_train = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å–ª–æ—è.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Å–ª–æ–π.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, _lr: float) -> None:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è.\n",
    "\n",
    "        –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è —Å–ª–æ—ë–≤ –±–µ–∑ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "\n",
    "        Args:\n",
    "            _lr (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Å–ª–æ–π –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è.\"\"\"\n",
    "        self.is_train = True\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        \"\"\"–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Å–ª–æ–π –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\"\"\"\n",
    "        self.is_train = False\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞—Ñ—Ñ–∏–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ–æ—Ä–º—É–ª–µ:\n",
    "        z = W @ x + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, weight_init: str = \"previous\"\n",
    "    ) -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): –ß–∏—Å–ª–æ –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "            out_dim (int): –ß–∏—Å–ª–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.w: np.ndarray | None = None\n",
    "        self.b = np.zeros((out_dim, 1))\n",
    "\n",
    "        self.x: np.ndarray | None = None\n",
    "        self.z: np.ndarray | None = None\n",
    "\n",
    "        self.dw: np.ndarray | None = None\n",
    "        self.db: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (in_dim, batch_size).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (out_dim, batch_size).\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise RuntimeError(\"Cannot call forward() before initializing weights.\")\n",
    "\n",
    "        self.x = x\n",
    "        self.z = self.w @ x + self.b\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∏ –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.x is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        self.dw = grad @ self.x.T\n",
    "        self.db = np.sum(grad, axis=1, keepdims=True)\n",
    "\n",
    "        grad_x = self.w.T @ grad\n",
    "        return grad_x\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"–û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª–æ—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "        Args:\n",
    "            lr (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "        \"\"\"\n",
    "        if self.db is None or self.dw is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "\n",
    "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω (0, 1) –ø–æ —Ñ–æ—Ä–º—É–ª–µ:\n",
    "        œÉ(z) = 1 / (1 + exp(-z))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"kaiming_uniform\") -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–π —Å–∏–≥–º–æ–∏–¥–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,\n",
    "                –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.y: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ägrad_x—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å–∏–≥–º–æ–∏–¥–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø—Ä–µ–¥–∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–∏–≥–º–æ–∏–¥—ã.\n",
    "        \"\"\"\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å–∏–≥–º–æ–∏–¥—ã –ø–æ –≤—Ö–æ–¥—É.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.y is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        return grad * self.y * (1 - self.y)\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU (Rectified Linear Unit).\n",
    "\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:\n",
    "        f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"xavier_uniform\") -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–π ReLU.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,\n",
    "                –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.mask: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é ReLU –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è ReLU.\n",
    "        \"\"\"\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç ReLU –ø–æ –≤—Ö–æ–¥—É.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.mask is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        return grad * self.mask\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax.\n",
    "\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:\n",
    "        f(z) = esp(z) / sum(exp(z))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init: str = \"kaiming_uniform\") -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–π Softmax.\n",
    "\n",
    "        Args:\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,\n",
    "                –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.y: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é Softmax –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è Softmax.\n",
    "        \"\"\"\n",
    "        shifted_x = x - np.max(x, axis=0, keepdims=True)\n",
    "        exp_x = np.exp(shifted_x)\n",
    "        self.y = exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç Softmax –ø–æ –≤—Ö–æ–¥—É.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.y is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        batch_size = self.y.shape[0]\n",
    "        dx = np.zeros_like(grad)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            s = self.y[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(s) - np.dot(s, s.T)\n",
    "            dx[i] = np.dot(jacobian, grad[i])\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    \"\"\"–°–ª–æ–π Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "\n",
    "    –í —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ –∑–∞–Ω—É–ª—è–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤—Ö–æ–¥–∞ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é p\n",
    "    –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p: float, weight_init: str = \"previous\") -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–π Dropout.\n",
    "\n",
    "        Args:\n",
    "            p (float): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞–Ω—É–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–∞.\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,\n",
    "                –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.p = p\n",
    "        self.mask: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç Dropout –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–∞—Å–∫–∏ Dropout.\n",
    "        \"\"\"\n",
    "        if not self.is_train or self.p == 0:\n",
    "            return x\n",
    "\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p).astype(x.dtype)\n",
    "        self.mask /= 1.0 - self.p\n",
    "\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Å–∫—É Dropout –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç—É –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.mask is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        return grad * self.mask\n",
    "\n",
    "\n",
    "class Batchnorm(Layer):\n",
    "    \"\"\"–°–ª–æ–π Batch Normalization (–¥–ª—è 2D —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Ñ–æ—Ä–º—ã (features, batch_size)).\n",
    "\n",
    "    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –±–∞—Ç—á—É:\n",
    "        x_hat = (x - mean) / sqrt(var + eps)\n",
    "        y = gamma * x_hat + beta\n",
    "\n",
    "    –í–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ—Ü–µ–Ω–∫–∏ mean/var.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        eps: float = 1e-5,\n",
    "        momentum: float = 0.9,\n",
    "        weight_init: str = \"previous\",\n",
    "    ) -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–π Dropout.\n",
    "\n",
    "        Args:\n",
    "            n_in (int): –ß–∏—Å–ª–æ –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "            eps (float): –ú–∞–ª–æ–µ —á–∏—Å–ª–æ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª–µ.\n",
    "            momentum (float): –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –¥–ª—è\n",
    "                –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è running_mean –∏ running_var.\n",
    "            weight_init (str): –ò–º—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏,\n",
    "                –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.\n",
    "        \"\"\"\n",
    "        super().__init__(weight_init)\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = np.ones((n_in, 1))\n",
    "        self.beta = np.zeros((n_in, 1))\n",
    "\n",
    "        self.dgamma: np.ndarray | None = None\n",
    "        self.dbeta: np.ndarray | None = None\n",
    "\n",
    "        self.running_mean = np.zeros((n_in, 1))\n",
    "        self.running_var = np.ones((n_in, 1))\n",
    "\n",
    "        self.x: np.ndarray | None = None\n",
    "        self.x_hat: np.ndarray | None = None\n",
    "        self.mean: np.ndarray | None = None\n",
    "        self.var: np.ndarray | None = None\n",
    "        self.inv_std: np.ndarray | None = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç Batchnorm –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è Batchnorm.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "\n",
    "        if self.is_train:\n",
    "            self.mean = np.mean(x, axis=1, keepdims=True)\n",
    "            self.var = np.var(x, axis=1, keepdims=True)\n",
    "            self.inv_std = 1.0 / np.sqrt(self.var + self.eps)\n",
    "\n",
    "            self.x_hat = (x - self.mean) * self.inv_std\n",
    "\n",
    "            self.running_mean = (\n",
    "                self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            )\n",
    "            self.running_var = (\n",
    "                self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "            )\n",
    "        else:\n",
    "            inv_std = 1.0 / np.sqrt(self.running_var + self.eps)\n",
    "            self.x_hat = (x - self.running_mean) * inv_std\n",
    "\n",
    "        return self.gamma * self.x_hat + self.beta\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç Batchnorm –ø–æ –≤—Ö–æ–¥—É.\n",
    "\n",
    "        Args:\n",
    "            grad (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–ª–æ—è.\n",
    "        \"\"\"\n",
    "        if self.x is None or self.x_hat is None:\n",
    "            raise RuntimeError(\"Cannot call backward() before forward().\")\n",
    "\n",
    "        # grads for affine part\n",
    "        self.dbeta = np.sum(grad, axis=1, keepdims=True)\n",
    "        self.dgamma = np.sum(grad * self.x_hat, axis=1, keepdims=True)\n",
    "\n",
    "        dxhat = grad * self.gamma\n",
    "\n",
    "        if not self.is_train:\n",
    "            inv_std = 1.0 / np.sqrt(self.running_var + self.eps)\n",
    "            return dxhat * inv_std\n",
    "\n",
    "        B = self.x.shape[1]\n",
    "        inv_std = self.inv_std\n",
    "\n",
    "        sum_dxhat = np.sum(dxhat, axis=1, keepdims=True)\n",
    "        sum_dxhat_xhat = np.sum(dxhat * self.x_hat, axis=1, keepdims=True)\n",
    "\n",
    "        return (\n",
    "            (1.0 / B) * inv_std * (B * dxhat - sum_dxhat - self.x_hat * sum_dxhat_xhat)\n",
    "        )\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"–û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª–æ—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "        Args:\n",
    "            lr (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "        \"\"\"\n",
    "        if self.dgamma is None or self.dbeta is None:\n",
    "            raise RuntimeError(\"Cannot call step() before backward().\")\n",
    "\n",
    "        self.gamma -= lr * self.dgamma\n",
    "        self.beta -= lr * self.dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c645c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –Ω–∞–±–æ—Ä —Å–ª–æ—ë–≤.\n",
    "\n",
    "    –£–ø—Ä–∞–≤–ª—è–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤ –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—ë–≤, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞,\n",
    "    –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –¢–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç\n",
    "    –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ—ë–≤ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\n",
    "\n",
    "    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å —É—á—ë—Ç–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, —É–∫–∞–∑–∞–Ω–Ω–æ–π –≤ —Å–ª–æ–µ. –ï—Å–ª–∏ –¥–ª—è —Å–ª–æ—è\n",
    "    –∑–∞–¥–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ \"previous\", –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, —É–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–Ω–∞—è\n",
    "    –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω–æ–≥–æ —Å–ª–æ—è, –∫–æ—Ç–æ—Ä—ã–π —è–≤–Ω–æ —É–∫–∞–∑–∞–ª —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.\n",
    "\n",
    "    Attributes:\n",
    "        layers (list[Layer]): –°–ø–∏—Å–æ–∫ —Å–ª–æ—ë–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.\n",
    "        _rng (np.random.Generator): –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.\n",
    "        w_init (WeightInitializer): –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–µ—Å–æ–≤.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: list[Layer], random_state: int | None = None) -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤.\n",
    "\n",
    "        Args:\n",
    "            layers (list[Layer]): –°–ª–æ–∏ —Å–µ—Ç–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.\n",
    "            random_state (int | None): –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª.\n",
    "                –ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self._rng = np.random.default_rng(random_state)\n",
    "\n",
    "        self.w_init = WeightInitializer(self._rng)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—ë–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏.\n",
    "\n",
    "        –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ —Å–ª–æ—è–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "        –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—ë–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ weight_init. –ï—Å–ª–∏ —Å–ª–æ–π –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        \"previous\", —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        \"\"\"\n",
    "        prev_init = \"random_uniform\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            if isinstance(layer, Linear):\n",
    "                init_name = prev_init\n",
    "                if layer.weight_init != \"previous\":\n",
    "                    init_name = layer.weight_init\n",
    "                layer.w = self.w_init(init_name, (layer.out_dim, layer.in_dim))\n",
    "\n",
    "            layer_init = layer.weight_init\n",
    "            if layer_init != \"previous\":\n",
    "                prev_init = layer_init\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ —Å–µ—Ç–∏.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –í—ã—Ö–æ–¥ —Å–µ—Ç–∏ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Å–ª–æ—ë–≤.\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ —Å–µ—Ç–∏.\n",
    "\n",
    "        Args:\n",
    "            grad_out (np.ndarray): –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É —Å–µ—Ç–∏.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤—Ö–æ–¥—É —Å–µ—Ç–∏.\n",
    "        \"\"\"\n",
    "        grad = grad_out\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"–û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å–µ—Ö —Å–ª–æ—ë–≤ —Å–µ—Ç–∏ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "        Args:\n",
    "            lr (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(lr)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ —Å–ª–æ–∏ —Å–µ—Ç–∏ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        \"\"\"–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ —Å–ª–æ–∏ —Å–µ—Ç–∏ –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe4f22",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be0076b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | train_loss:  0.3094 | train_acc:  0.8681 | test_loss:  0.2915 | test_acc:  0.8860\n",
      "Epoch 010 | train_loss:  0.1013 | train_acc:  0.9714 | test_loss:  0.0839 | test_acc:  0.9649\n",
      "Epoch 020 | train_loss:  0.0834 | train_acc:  0.9780 | test_loss:  0.0897 | test_acc:  0.9737\n",
      "Epoch 030 | train_loss:  0.0711 | train_acc:  0.9780 | test_loss:  0.0869 | test_acc:  0.9649\n",
      "Epoch 040 | train_loss:  0.0730 | train_acc:  0.9758 | test_loss:  0.0859 | test_acc:  0.9649\n",
      "Epoch 050 | train_loss:  0.0587 | train_acc:  0.9824 | test_loss:  0.0926 | test_acc:  0.9561\n",
      "Epoch 060 | train_loss:  0.0559 | train_acc:  0.9868 | test_loss:  0.1461 | test_acc:  0.9561\n",
      "Epoch 070 | train_loss:  0.0523 | train_acc:  0.9802 | test_loss:  0.0979 | test_acc:  0.9561\n",
      "Epoch 080 | train_loss:  0.0515 | train_acc:  0.9846 | test_loss:  0.1103 | test_acc:  0.9649\n",
      "Epoch 090 | train_loss:  0.0485 | train_acc:  0.9824 | test_loss:  0.1122 | test_acc:  0.9561\n",
      "Epoch 100 | train_loss:  0.0503 | train_acc:  0.9846 | test_loss:  0.0934 | test_acc:  0.9649\n",
      "Epoch 110 | train_loss:  0.0507 | train_acc:  0.9868 | test_loss:  0.0897 | test_acc:  0.9649\n",
      "Epoch 120 | train_loss:  0.0467 | train_acc:  0.9868 | test_loss:  0.0911 | test_acc:  0.9561\n",
      "Epoch 130 | train_loss:  0.0465 | train_acc:  0.9890 | test_loss:  0.0822 | test_acc:  0.9649\n",
      "Epoch 140 | train_loss:  0.0438 | train_acc:  0.9868 | test_loss:  0.0780 | test_acc:  0.9561\n",
      "Epoch 150 | train_loss:  0.0430 | train_acc:  0.9890 | test_loss:  0.0826 | test_acc:  0.9561\n",
      "Epoch 160 | train_loss:  0.0369 | train_acc:  0.9890 | test_loss:  0.0966 | test_acc:  0.9561\n",
      "Epoch 170 | train_loss:  0.0403 | train_acc:  0.9890 | test_loss:  0.0778 | test_acc:  0.9561\n",
      "Epoch 180 | train_loss:  0.0391 | train_acc:  0.9890 | test_loss:  0.0915 | test_acc:  0.9561\n",
      "Epoch 190 | train_loss:  0.0430 | train_acc:  0.9912 | test_loss:  0.0780 | test_acc:  0.9649\n",
      "Epoch 200 | train_loss:  0.0384 | train_acc:  0.9912 | test_loss:  0.0893 | test_acc:  0.9561\n"
     ]
    }
   ],
   "source": [
    "my_net = NeuralNetwork(\n",
    "    [\n",
    "        Linear(30, 64),\n",
    "        Batchnorm(64),\n",
    "        Dropout(0.3),\n",
    "        ReLU(),\n",
    "        Linear(64, 32),\n",
    "        Batchnorm(32),\n",
    "        Dropout(0.2),\n",
    "        ReLU(),\n",
    "        Linear(32, 1),\n",
    "        Sigmoid(),\n",
    "    ],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "learning_rate = 0.05\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "N = X_train.shape[1]\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    my_net.train()\n",
    "\n",
    "    idx = np.random.permutation(N)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch_idx = idx[start : start + batch_size]\n",
    "        x = X_train[:, batch_idx]\n",
    "        yb = y_train[:, batch_idx]\n",
    "        B = x.shape[1]\n",
    "\n",
    "        y_hat = my_net.forward(x)\n",
    "        grad = (y_hat - yb) / B\n",
    "        my_net.backward(grad)\n",
    "        my_net.step(learning_rate)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        my_net.eval()\n",
    "\n",
    "        y_train_pred = my_net.forward(X_train).ravel()\n",
    "        y_test_pred = my_net.forward(X_test).ravel()\n",
    "\n",
    "        y_train_true = y_train.ravel()\n",
    "        y_test_true = y_test.ravel()\n",
    "\n",
    "        train_loss = log_loss(y_train_true, y_train_pred)\n",
    "        train_acc = accuracy_score(y_train_true, y_train_pred > 0.5)\n",
    "        test_loss = log_loss(y_test_true, y_test_pred)\n",
    "        test_acc = accuracy_score(y_test_true, y_test_pred > 0.5)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss: {train_loss:7.4f} | \"\n",
    "            f\"train_acc: {train_acc:7.4f} | \"\n",
    "            f\"test_loss: {test_loss:7.4f} | \"\n",
    "            f\"test_acc: {test_acc:7.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4709edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | train_loss:  0.4764 | train_acc:  0.9670 | test_loss:  0.4865 | test_acc:  0.9561\n",
      "Epoch 010 | train_loss:  0.0977 | train_acc:  0.9824 | test_loss:  0.1359 | test_acc:  0.9649\n",
      "Epoch 020 | train_loss:  0.0612 | train_acc:  0.9824 | test_loss:  0.1018 | test_acc:  0.9737\n",
      "Epoch 030 | train_loss:  0.0559 | train_acc:  0.9868 | test_loss:  0.0782 | test_acc:  0.9825\n",
      "Epoch 040 | train_loss:  0.0500 | train_acc:  0.9912 | test_loss:  0.1042 | test_acc:  0.9561\n",
      "Epoch 050 | train_loss:  0.0310 | train_acc:  0.9868 | test_loss:  0.0774 | test_acc:  0.9649\n",
      "Epoch 060 | train_loss:  0.0332 | train_acc:  0.9890 | test_loss:  0.0855 | test_acc:  0.9649\n",
      "Epoch 070 | train_loss:  0.0286 | train_acc:  0.9956 | test_loss:  0.0766 | test_acc:  0.9649\n",
      "Epoch 080 | train_loss:  0.0259 | train_acc:  0.9934 | test_loss:  0.0636 | test_acc:  0.9825\n",
      "Epoch 090 | train_loss:  0.0282 | train_acc:  0.9934 | test_loss:  0.0818 | test_acc:  0.9649\n",
      "Epoch 100 | train_loss:  0.0209 | train_acc:  0.9934 | test_loss:  0.0640 | test_acc:  0.9825\n",
      "Epoch 110 | train_loss:  0.0219 | train_acc:  0.9934 | test_loss:  0.0664 | test_acc:  0.9649\n",
      "Epoch 120 | train_loss:  0.0236 | train_acc:  0.9978 | test_loss:  0.0739 | test_acc:  0.9649\n",
      "Epoch 130 | train_loss:  0.0165 | train_acc:  0.9978 | test_loss:  0.0638 | test_acc:  0.9737\n",
      "Epoch 140 | train_loss:  0.0304 | train_acc:  0.9890 | test_loss:  0.0705 | test_acc:  0.9649\n",
      "Epoch 150 | train_loss:  0.0198 | train_acc:  0.9978 | test_loss:  0.0805 | test_acc:  0.9649\n",
      "Epoch 160 | train_loss:  0.0181 | train_acc:  0.9978 | test_loss:  0.0699 | test_acc:  0.9649\n",
      "Epoch 170 | train_loss:  0.0163 | train_acc:  0.9978 | test_loss:  0.0675 | test_acc:  0.9825\n",
      "Epoch 180 | train_loss:  0.0150 | train_acc:  0.9978 | test_loss:  0.0661 | test_acc:  0.9825\n",
      "Epoch 190 | train_loss:  0.0156 | train_acc:  0.9934 | test_loss:  0.0652 | test_acc:  0.9737\n",
      "Epoch 200 | train_loss:  0.0136 | train_acc:  1.0000 | test_loss:  0.0622 | test_acc:  0.9737\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    net.train()\n",
    "    for xb, yb in train_loader:\n",
    "        # forward\n",
    "        logits = net(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # backward\n",
    "        net.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        for param in net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.data -= lr * param.grad\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            train_logits = net(X_train_t)\n",
    "            test_logits = net(X_test_t)\n",
    "\n",
    "            train_prob = torch.sigmoid(train_logits).cpu().numpy().ravel()\n",
    "            test_prob = torch.sigmoid(test_logits).cpu().numpy().ravel()\n",
    "\n",
    "            y_train_np = y_train_t.cpu().numpy().ravel()\n",
    "            y_test_np = y_test_t.cpu().numpy().ravel()\n",
    "\n",
    "            train_loss = log_loss(y_train_np, train_prob)\n",
    "            test_loss = log_loss(y_test_np, test_prob)\n",
    "            train_acc = accuracy_score(y_train_np, train_prob > 0.5)\n",
    "            test_acc = accuracy_score(y_test_np, test_prob > 0.5)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss: {train_loss:7.4f} | \"\n",
    "            f\"train_acc: {train_acc:7.4f} | \"\n",
    "            f\"test_loss: {test_loss:7.4f} | \"\n",
    "            f\"test_acc: {test_acc:7.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afdb48",
   "metadata": {},
   "source": [
    "### üìä Comparing Algorithm Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2628c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net.eval()\n",
    "torch_pred = torch.sigmoid(net(X_test_t)).detach().cpu().numpy().ravel()\n",
    "my_pred = my_net.forward(X_test).ravel()\n",
    "\n",
    "predictions = {\n",
    "    \"torch\": torch_pred,\n",
    "    \"my\": my_pred,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec1cef00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>torch</th>\n",
       "      <th>my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.985507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.964539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.998016</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              torch        my\n",
       "Accuracy   0.973684  0.956140\n",
       "Precision  0.972603  0.985507\n",
       "Recall     0.986111  0.944444\n",
       "F1-score   0.979310  0.964539\n",
       "ROC AUC    0.998016  0.997024"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_models(predictions, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "faa2e09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrices({k: v > 0.5 for k, v in predictions.items()}, y_test.ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
